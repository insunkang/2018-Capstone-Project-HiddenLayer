{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import Activation, UpSampling2D, Conv2D\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import pyaudio\n",
    "import wave\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import MySQLdb\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from utils import audio_tools as audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data generated with preprocess_audio.py\n",
    "DATAPATH = \"/Users/jin/Desktop/DATA/training_data.npz\"\n",
    "\n",
    "# trained model paths for generator and discriminator\n",
    "D_WEIGHT_PATH =\"/Users/jin/Desktop/DATA/discriminator_epoch_1200_-2.32.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category info\n",
    "CATEGORIES = np.load(DATAPATH)[\"category_names\"]\n",
    "nb_categories = len(CATEGORIES)\n",
    "\n",
    "# for denomalizing mel_spectrogram\n",
    "mel_means = np.load(DATAPATH)[\"mean\"]\n",
    "mel_stds = np.load(DATAPATH)[\"std\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator(nb_categories):\n",
    "    D = 64 # model size\n",
    "    input_data = Input(shape=(128, 128, 1))\n",
    "    x = Conv2D(D, (5, 5), strides=(2,2), padding='same')(input_data)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(D * 2, (5, 5), strides=(2,2), kernel_initializer='he_normal',padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    real_fake = Dense(1, kernel_initializer='he_normal',name='real_fake')(x)\n",
    "    \n",
    "    categories = Dense(nb_categories,kernel_initializer='he_normal',name='categories',activation='softmax')(x)\n",
    "    \n",
    "    model = Model(input_data, [real_fake, categories])\n",
    "    return model\n",
    "\n",
    "#generator = make_generator()\n",
    "discriminator = make_discriminator(nb_categories)\n",
    "\n",
    "# load weights\n",
    "#generator.load_weights(G_WEIGHT_PATH)\n",
    "discriminator.load_weights(D_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "import matplotlib.pyplot as plt\n",
    "reload(audio)\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# check if the discriminator thinks the generated sound as real sound\n",
    "CONFIDENCE_THRESH = 0.80\n",
    "\n",
    "def denormalize(norm_s):\n",
    "    assert norm_s.shape[0] == mel_means.shape[0]\n",
    "    Y = (norm_s * (3.0 * mel_stds)) + mel_means\n",
    "    return Y\n",
    "\n",
    "def save_audio(y, path, category=None, stereo=False):\n",
    "    _s = np.squeeze(y)\n",
    "    \n",
    "    if stereo:\n",
    "        assert _s.shape[0] == 2\n",
    "        channels = []\n",
    "        for s in _s:\n",
    "            s = denormalize(s)\n",
    "            w = audio.inv_melspectrogram(s)\n",
    "            channels.append(w)\n",
    "        w = np.vstack(channels)\n",
    "        w = np.transpose(w)\n",
    "        audio.save_wav(w, path)    \n",
    "    else:\n",
    "        s = denormalize(_s)\n",
    "        w = audio.inv_melspectrogram(s)\n",
    "        audio.save_wav(w, path)\n",
    "    \n",
    "def show_spec(S, display=False):    \n",
    "    if S.shape[0] == 2:\n",
    "        S = S[0]\n",
    "    S = np.squeeze(S)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(S)\n",
    "    plt.show()\n",
    "    \n",
    "    if display:\n",
    "        path = \"/tmp/test.wav\"\n",
    "        save_audio(S, path)\n",
    "        IPython.display.display(Audio(path))\n",
    "\n",
    "def classify_drums(w, thresh=CONFIDENCE_THRESH):\n",
    "    w = np.squeeze(w)\n",
    "    w = w[np.newaxis, :, :, np.newaxis]\n",
    "    r, p = discriminator.predict([w])\n",
    "    if float(r) > thresh:\n",
    "        return np.argmax(p)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def generate_random_sound(max_try= 100):\n",
    "\n",
    "    for i in range(max_try):\n",
    "        seed = np.random.rand(1, 100)\n",
    "        w =   generator.predict(seed)\n",
    "        id_category = classify_drums(w[0])\n",
    "        category =  CATEGORIES[id_category]\n",
    "        if  id_category >= 0:\n",
    "            save_audio(w, \"./drums_%s.wav\" % category, category=category)\n",
    "            print category\n",
    "            show_spec(w, True)\n",
    "            break\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "def classify_sound(w, thresh=CONFIDENCE_THRESH):\n",
    "    y, sr = librosa.core.load(w, sr =16000)\n",
    "    \n",
    "    db_mel = audio.melspectrogram(y)\n",
    "    assert db_mel.shape[0] == IMAGE_SIZE\n",
    "    \n",
    "    dummy = np.ones((IMAGE_SIZE, IMAGE_SIZE))*(-80)\n",
    "    db_mel = np.hstack((db_mel , dummy))\n",
    "    db_mel = db_mel[:,:IMAGE_SIZE]\n",
    "    \n",
    "    w = np.squeeze(db_mel)\n",
    "    w = w[np.newaxis,:,:, np.newaxis]\n",
    "    r,p =discriminator.predict([w])\n",
    "    id_category = np.argmax(p)\n",
    "    category = CATEGORIES[id_category]\n",
    "    \n",
    "    return float(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scanning splited files\n",
    "def calaverage(dirname):\n",
    "    try:\n",
    "        count = 0\n",
    "        summ = 0\n",
    "        filenames = os.listdir(dirname)\n",
    "        for filename in filenames:\n",
    "            full_filename = os.path.join(dirname, filename)\n",
    "            ext = os.path.splitext(full_filename)[-1]\n",
    "            if ext == '.wav':\n",
    "                count +=1\n",
    "                a = int(classify_sound(full_filename))\n",
    "                summ = summ + a\n",
    "            \n",
    "        average = summ/count\n",
    "    except:\n",
    "        average = -99\n",
    "        \n",
    "    return average\n",
    "\n",
    "def remove_sound():\n",
    "    for root, dirs, files in os.walk('seungjin'):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 음향 데이터 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d599c8e724bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pyaudio.pyc\u001b[0m in \u001b[0;36mstop_stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_running\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True :\n",
    "     \n",
    "    CHUNK =128\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    RECORD_SECONDS = 3\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    stream = p.open(format=FORMAT,\n",
    "                   channels=CHANNELS,\n",
    "                   rate=RATE,\n",
    "                   input=True,\n",
    "                   frames_per_buffer=CHUNK)\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        \n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "        \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    ############################################################\n",
    "    try:\n",
    "        \n",
    "        sound_file = AudioSegment.from_wav(\"output.wav\")\n",
    "        Promotion = split_on_silence(sound_file, min_silence_len= 100,\n",
    "                                silence_thresh= -17 )\n",
    "        for i, jin in enumerate(Promotion):\n",
    "            splited_file = \"seungjin/jin{0}.wav\".format(i)\n",
    "            jin.export(splited_file, format=\"wav\")\n",
    "\n",
    "        ###Sound Classification###############################\n",
    "\n",
    "        if calaverage(\"seungjin\")>=150:\n",
    "            ave= calaverage(\"seungjin\")\n",
    "            \n",
    "        else :\n",
    "            remove_sound()\n",
    "            \n",
    "            sound_file = AudioSegment.from_wav(\"output.wav\")\n",
    "            Promotion = split_on_silence(sound_file, min_silence_len= 100,\n",
    "                                     silence_thresh= -22 )\n",
    "            for i, jin in enumerate(Promotion):\n",
    "                splited_file = \"seungjin/jin{0}.wav\".format(i)\n",
    "                jin.export(splited_file, format=\"wav\")\n",
    "            \n",
    "            ave= calaverage(\"seungjin\")\n",
    "        \n",
    "        #     ###### upload ave on database ##########\n",
    "        db = MySQLdb.connect(\"smurf1213.cafe24.com\",\"smurf1213\",\"1q2w3e4r!\",\"smurf1213\" )\n",
    "        cursor = db.cursor()\n",
    "        sql = \"INSERT INTO RaspberryPi(value) \\\n",
    "               VALUES ('%d')\" % (ave)\n",
    "        cursor.execute(sql)\n",
    "        db.commit()\n",
    "        db.close()   \n",
    "        \n",
    "\n",
    "    finally:\n",
    "        remove_sound()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
